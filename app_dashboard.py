
# -*- coding: utf-8 -*-
"""
ì¢…ëª©ë¦¬í¬íŠ¸ í‰ê°€ ëŒ€ì‹œë³´ë“œ (Aì•ˆ: st.data_editor ê¸°ë°˜, ì»´í¬ë„ŒíŠ¸/iframe ì œê±°)
- íƒ­1: ğŸ† ì¢…ëª©ë¦¬í¬íŠ¸ í‰ê°€ ë­í‚¹ë³´ë“œ  â†’ ì²´í¬ë°•ìŠ¤ ë‹¨ì¼ ì„ íƒ + ì¿¼ë¦¬ìŠ¤íŠ¸ë§ ì‹±í¬ â†’ í•˜ë‹¨ ìƒì„¸
- íƒ­2: ğŸ” ì¢…ëª©ë³„ ê²€ìƒ‰                 â†’ ì²´í¬ë°•ìŠ¤ ë‹¨ì¼ ì„ íƒ + ì¿¼ë¦¬ìŠ¤íŠ¸ë§ ì‹±í¬ â†’ í•˜ë‹¨ ìƒì„¸
(ìƒˆ íƒ­ ì—´ê¸° ì—†ìŒ)
"""

# ==== ìƒë‹¨ ê³µí†µ ë¶€íŠ¸ìŠ¤íŠ¸ë© (ëª¨ë“  st.* í˜¸ì¶œë³´ë‹¤ ë¨¼ì €) ====
import os
os.environ.setdefault("STREAMLIT_CACHE_DIR", "/tmp/streamlit-cache")
os.environ.setdefault("STREAMLIT_BROWSER_GATHER_USAGE_STATS", "false")
os.environ.setdefault("STREAMLIT_SERVER_FILE_WATCHER_TYPE", "none")

import re
import datetime as dt
from typing import List, Dict, Any
import urllib.parse as _up

import pandas as pd
import pytz
import streamlit as st
from google.cloud import firestore


# 1) ê°€ì¥ ë¨¼ì € page_config
st.set_page_config(page_title="í•œêµ­í´ë¦¬í…ëŒ€í•™ ìŠ¤ë§ˆíŠ¸ê¸ˆìœµê³¼", layout="wide")


# ====== ìœ í‹¸ ======
TABLE_SCROLL_HEIGHT = 520
KST = pytz.timezone("Asia/Seoul")

def naver_item_url(ticker: str | None) -> str:
    t = (ticker or "").strip()
    if not t or len(t) != 6:
        return "https://finance.naver.com"
    return f"https://finance.naver.com/item/main.naver?code={t}"

def last_close_from_horizons(rec: dict) -> float | None:
    horizons = rec.get("horizons") or []
    if not horizons:
        return None
    try:
        horizons = sorted(horizons, key=lambda h: h.get("end_date",""))
    except Exception:
        pass
    last_close = None
    for h in horizons:
        ps = h.get("price_sample") or []
        closes = [p.get("close") for p in ps if p.get("close") is not None]
        if closes:
            last_close = float(closes[-1])
    return last_close

def normalize_broker_name(name: str) -> str:
    if not name:
        return ""
    name = re.sub(r"\s+", "", name)
    name = re.sub(r"[&/\.#\[\]]", "_", name)
    name = re.sub(r"[\(\)\[\]]", "", name)
    return name.strip()

def normalize_analyst_name(name: str) -> str:
    if not name:
        return ""
    return re.sub(r"\s+", "", str(name)).strip()

def format_ts(ts, tz=KST):
    if not ts:
        return "-"
    try:
        dt_utc = ts if isinstance(ts, dt.datetime) else ts.to_datetime()
    except Exception:
        try:
            dt_utc = dt.datetime.fromisoformat(str(ts))
        except Exception:
            return str(ts)
    if dt_utc.tzinfo is None:
        dt_utc = dt_utc.replace(tzinfo=dt.timezone.utc)
    return dt_utc.astimezone(tz).strftime("%Y-%m-%d %H:%M:%S %Z")

# ì¿¼ë¦¬ìŠ¤íŠ¸ë§ ìœ í‹¸ (íƒ­1: pick, íƒ­2: pick2)
def _build_pick_token(analyst: str, broker: str) -> str:
    return _up.quote(f"{analyst}||{broker}", safe="")

def _parse_pick_token(token: str):
    try:
        if not token:
            return None
        s = _up.unquote(token)
        analyst, broker = s.split("||", 1)
        return analyst, broker
    except Exception:
        return None

def _get_qp(name: str) -> str | None:
    v = st.query_params.get(name)
    if isinstance(v, list):
        return v[0] if v else None
    return v

def _set_qp(name: str, value: str | None):
    qp = dict(st.query_params)
    if value is None:
        qp.pop(name, None)
    else:
        qp[name] = value
    st.query_params.clear()
    st.query_params.update(qp)


# -----------------------------
# Firestore Client
# -----------------------------
@st.cache_resource(show_spinner=False)
def get_db():
    return firestore.Client()

@st.cache_data(show_spinner=False, ttl=300)
def fetch_titles_for_records(records: list[dict]) -> dict[str, str]:
    db = get_db()
    ids = []
    for r in records:
        rid = str(r.get("report_id") or "").strip()
        if rid:
            ids.append(rid)
    ids = sorted(set(ids))
    if not ids:
        return {}

    refs = [db.collection("analyst_reports").document(rid) for rid in ids]
    try:
        snaps = db.get_all(refs)
    except Exception:
        return {}

    out = {}
    for s in snaps:
        if not s.exists:
            continue
        d = s.to_dict() or {}
        t = (d.get("title") or d.get("report_title") or d.get("subject") or "").strip()
        if t:
            out[s.id] = t
    return out

def _pick_title(rec: dict) -> str:
    for k in ["title", "report_title", "subject", "report_subject", "headline", "name"]:
        t = (rec.get(k) or "").strip()
        if t:
            return t
    return ""

def _title_from_map_or_fields(rec: dict, title_map: dict[str, str] | None = None) -> str:
    rid = str(rec.get("report_id") or "").strip()
    if title_map and rid in title_map:
        return title_map[rid]
    t = _pick_title(rec)
    if t:
        return t
    stock = (rec.get("stock") or "").strip()
    rdate = (rec.get("report_date") or "").strip()
    return f"{stock or 'ë¦¬í¬íŠ¸'} ({rdate or '-'})"


# -----------------------------
# ë°ì´í„° ë¡œë“œ
# -----------------------------
@st.cache_data(show_spinner=False, ttl=60)
def load_analyst_docs(date_from: dt.date, date_to: dt.date,
                      brokers: List[str] | None, limit: int = 3000) -> List[Dict[str, Any]]:
    db = get_db()
    docs_raw = []
    server_filtered = False
    try:
        q = db.collection_group("by_analyst")
        want_min = date_from.isoformat() if date_from else None
        want_max_excl = (date_to + dt.timedelta(days=1)).isoformat() if date_to else None

        q = db.collection_group("by_analyst")
        if want_min:     q = q.where("report_date", ">=", want_min)
        if want_max_excl:q = q.where("report_date", "<",  want_max_excl)
        q = q.limit(limit)
        docs_raw = list(q.stream())
        server_filtered = True
    except Exception:
        q = db.collection_group("by_analyst").limit(limit)
        docs_raw = list(q.stream())

    broker_set = {normalize_broker_name(b) for b in (brokers or [])} or None
    out: List[Dict[str, Any]] = []
    for d in docs_raw:
        x = d.to_dict() or {}
        broker_norm = normalize_broker_name((x.get("broker") or "").strip())
        if broker_norm == "í•œêµ­IRí˜‘ì˜íšŒ" or "í‰ê°€" in broker_norm:
            continue
        if broker_set and broker_norm not in broker_set:
            continue
        x["broker"] = broker_norm
        raw_name = (x.get("analyst_name") or x.get("analyst") or "").strip()
        x["analyst_name"] = raw_name
        x["analyst_name_norm"] = x.get("analyst_name_norm") or normalize_analyst_name(raw_name)

        if not server_filtered and (date_from or date_to):
            s = (x.get("report_date") or "").strip()
            if not s:
                continue
            try:
                d0 = dt.date.fromisoformat(s[:10])
            except Exception:
                continue
            if date_from and d0 < date_from:
                continue
            if date_to and d0 > date_to:
                continue

        if x.get("points_total") is None and x.get("horizons"):
            x["points_total"] = sum(float(h.get("points_total_this_horizon") or 0.0) for h in x.get("horizons") or [])
        out.append(x)
    return out

@st.cache_data(show_spinner=False, ttl=60)
def load_detail_for_analyst(analyst_name: str, broker: str,
                            date_from: dt.date | None, date_to: dt.date | None,
                            limit: int = 800) -> List[Dict[str, Any]]:
    db = get_db()
    out: List[Dict[str, Any]] = []
    broker_norm = normalize_broker_name(broker)
    target_norm = normalize_analyst_name(analyst_name)

    def _q(field: str, value: str):
        q = (db.collection_group("by_analyst")
               .where("broker", "==", broker_norm)
               .where(field, "==", value))
        try:
            if date_from: q = q.where("report_date", ">=", date_from.isoformat())
            if date_to:   q = q.where("report_date", "<=", date_to.isoformat())
        except Exception:
            pass
        q = q.limit(limit)
        docs = [z.to_dict() or {} for z in q.stream()]
        for z in docs:
            z["broker"] = normalize_broker_name(z.get("broker"))
            rn = (z.get("analyst_name") or z.get("analyst") or "").strip()
            z["analyst_name"] = rn
            z["analyst_name_norm"] = z.get("analyst_name_norm") or normalize_analyst_name(rn)
        return docs

    out.extend(_q("analyst_name_norm", target_norm))
    if not out:
        out.extend(_q("analyst_name", analyst_name))
        if target_norm != analyst_name:
            out.extend(_q("analyst_name", target_norm))

    seen = set(); uniq = []
    for r in out:
        key = (str(r.get("report_id","")), str(r.get("pdf_url","")))
        if key in seen:
            continue
        seen.add(key); uniq.append(r)
    uniq.sort(key=lambda r: r.get("report_date",""), reverse=True)
    return uniq


# -----------------------------
# ì§‘ê³„ & í…Œì´ë¸” ë³€í™˜
# -----------------------------
def make_rank_table(docs: list[dict], metric: str = "avg", min_reports: int = 2) -> pd.DataFrame:
    from collections import defaultdict
    agg = defaultdict(lambda: {"sum":0.0, "n":0, "name_disp":"", "broker":"", "first":None, "last":None})
    for x in docs:
        name_raw = (x.get("analyst_name") or x.get("analyst") or "").strip()
        name_norm = x.get("analyst_name_norm") or normalize_analyst_name(name_raw)
        broker = normalize_broker_name((x.get("broker") or "").strip())
        try:
            pts = float(x.get("points_total") or 0.0)
        except Exception:
            pts = 0.0
        rdate = (x.get("report_date") or "").strip()
        key = f"{name_norm}|{broker}"
        agg[key]["sum"] += pts
        agg[key]["n"] += 1
        if not agg[key]["name_disp"]:
            agg[key]["name_disp"] = name_raw or "(unknown)"
        agg[key]["broker"] = broker
        if rdate:
            if not agg[key]["first"] or rdate < agg[key]["first"]:
                agg[key]["first"] = rdate
            if not agg[key]["last"] or rdate > agg[key]["last"]:
                agg[key]["last"]  = rdate
    rows = []
    for _, v in agg.items():
        if v["n"] < min_reports: 
            continue
        score = (v["sum"]/v["n"]) if metric == "avg" else v["sum"]
        rows.append({
            "RankScore": round(score, 4),
            "Reports": v["n"],
            "Analyst": v["name_disp"],
            "Broker": v["broker"],
            "FirstReport": v["first"] or "",
            "LastReport": v["last"] or "",
        })
    if not rows:
        return pd.DataFrame(columns=["RankScore","Reports","Analyst","Broker","FirstReport","LastReport"])
    df = pd.DataFrame(rows).sort_values(["RankScore","Reports"], ascending=[False, False]).reset_index(drop=True)
    return df

def horizon_to_rows(rec: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Horizon ë ˆì½”ë“œë¥¼ í…Œì´ë¸”ìš© í–‰ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜.
    - target_price / price ìƒ˜í”Œì— None ë˜ëŠ” ë¬¸ìì—´ì´ ì„ì—¬ ìˆì–´ë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬.
    """
    def _to_float(x):
        if x is None:
            return None
        try:
            if isinstance(x, str):
                xs = x.strip().replace(',', '')
                if xs == '':
                    return None
                return float(xs)
            return float(x)
        except Exception:
            return None

    rows = []
    horizons = rec.get("horizons") or []
    rating_norm = rec.get("rating_norm") or rec.get("rating")
    target_price_raw = rec.get("target_price")
    target_price = _to_float(target_price_raw)

    for h in horizons:
        # ë°©í–¥/ìˆ˜ìµë¥ /ì •ë‹µ
        desired = None
        try:
            dtg = h.get("direction_target") or {}
            desired = int(dtg.get("desired")) if dtg.get("desired") is not None else None
        except Exception:
            desired = None

        ret = h.get("return_pct", None)
        ret = _to_float(ret)
        actual_sign = (1 if ret and ret > 0 else (-1 if ret and ret < 0 else (0 if ret == 0 else None)))
        correct = None
        if desired is not None and actual_sign is not None:
            if desired == 0: correct = (actual_sign == 0)
            else:            correct = (desired == actual_sign)

        # ê°€ê²© ìƒ˜í”Œ í†µê³„
        sample = h.get("price_sample") or []
        closes = [_to_float(p.get("close")) for p in sample if p.get("close") is not None]
        closes = [c for c in closes if c is not None]
        max_c = max(closes) if closes else None
        min_c = min(closes) if closes else None
        last_c = closes[-1] if closes else None

        # íŒŒìƒ ì§€í‘œ(ëª©í‘œê°€ ëŒ€ë¹„)
        target_return_pct = None
        if target_price not in (None, 0, 0.0) and last_c is not None:
            try:
                target_return_pct = round((last_c - target_price) / target_price * 100.0, 3)
            except Exception:
                target_return_pct = None

        max_reach_pct = None
        if target_price not in (None, 0, 0.0) and max_c is not None:
            try:
                max_reach_pct = round((max_c / target_price - 1.0) * 100.0, 3)
            except Exception:
                max_reach_pct = None

        rows.append({
            "ë³´ê³ ì„œì¼": rec.get("report_date",""),
            "horizon(ì¼)": h.get("horizon_days"),
            "ì˜ˆì¸¡í¬ì§€ì…˜": ("ìƒìŠ¹(buy)" if desired == 1 else ("í•˜ë½(sell)" if desired == -1 else "ì¤‘ë¦½(hold)")),
            "ë ˆì´íŒ…": rating_norm,
            "ì˜ˆì¸¡ëª©í‘œê°€": target_price if target_price is not None else rec.get("target_price"),
            "ë¦¬í¬íŠ¸ì¼ì— ë§¤ìˆ˜í–ˆì„ê²½ìš° ìˆ˜ìµë¥ %": None if ret is None else round(ret*100, 3),
            "ëª©í‘œê°€ëŒ€ë¹„ìˆ˜ìµë¥ %": target_return_pct,
            "ëª©í‘œê°€ëŒ€ë¹„ìµœëŒ€ë„ë‹¬%": max_reach_pct,
            "ë°©í–¥ì •ë‹µ": None if correct is None else ("âœ…" if correct else "âŒ"),
            "ë°©í–¥ì ìˆ˜": h.get("direction_point", 0.0),
            "ëª©í‘œê·¼ì ‘ì ìˆ˜": h.get("target_proximity_point", 0.0),
            "ëª©í‘œê°€HIT": h.get("hit_target_anytime", None),
            "êµ¬ê°„ìµœê³ ì¢…ê°€": max_c,
            "êµ¬ê°„ìµœì €ì¢…ê°€": min_c,
            "êµ¬ê°„ë§ˆì§€ë§‰ì¢…ê°€": last_c,
            "ì¢…ëª©": rec.get("stock",""),
            "í‹°ì»¤": rec.get("ticker",""),
            "ë¦¬í¬íŠ¸PDF": rec.get("pdf_url","") or rec.get("report_url",""),
            "ìƒì„¸í˜ì´ì§€": rec.get("detail_url",""),
            "ë¦¬í¬íŠ¸ì´ì ": rec.get("points_total", 0.0),
        })
    return rows

def match_stock(rec: dict, stock_q: str | None, ticker_q: str | None) -> bool:
    if not stock_q and not ticker_q:
        return True
    if ticker_q:
        t = (rec.get("ticker") or "").strip()
        if t and str(t).lower() == str(ticker_q).lower():
            return True
    if stock_q:
        name = (rec.get("stock") or "").strip()
        if name and str(stock_q).lower() in name.lower():
            return True
    return False

def get_last_updated_from_docs(docs):
    latest = None
    for d in docs:
        ts = d.get("updated_at")
        if not ts:
            continue
        try:
            t = ts if isinstance(ts, dt.datetime) else ts.to_datetime()
        except Exception:
            try:
                t = dt.datetime.fromisoformat(str(ts))
            except Exception:
                continue
        if latest is None or t > latest:
            latest = t
    return latest


# -----------------------------
# ë‹¨ì¼ì„ íƒ í…Œì´ë¸” ë Œë” (ì²´í¬ë°•ìŠ¤ + ì„¸ì…˜ ìƒíƒœ + ì¿¼ë¦¬íŒŒë¼ë¯¸í„° ì‹±í¬)
# -----------------------------
def render_single_select_table(df: pd.DataFrame, key: str, page_size: int, qp_key: str,
                               height: int = TABLE_SCROLL_HEIGHT, column_config: dict | None = None):
    """ì²´í¬ë°•ìŠ¤ 1ê°œë§Œ True ë˜ë„ë¡ ê°•ì œí•˜ê³ , ì„ íƒì´ ë°”ë€Œë©´ ì¿¼ë¦¬íŒŒë¼ë¯¸í„°(qp_key)ë¥¼ ì—…ë°ì´íŠ¸ í›„ rerun."""
    if df is None or df.empty:
        st.info("í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None

    total = len(df)
    total_pages = (total + page_size - 1) // page_size
    page = st.number_input("Page", 1, max(1, total_pages), 1, key=f"{key}_page")
    start, end = (page - 1) * page_size, min(page * page_size, total)
    st.caption(f"Page {page}/{total_pages} Â· Rows {start+1}-{end} / {total}")

    view = df.iloc[start:end].reset_index(drop=True).copy()

    # ì „ì—­ ì„ íƒ ì¸ë±ìŠ¤ ê´€ë¦¬
    sel_key = f"{key}__gidx"
    sel_gidx = st.session_state.get(sel_key, -1)

    # ì¿¼ë¦¬íŒŒë¼ë¯¸í„° ë³µì› ìš°ì„ 
    parsed = _parse_pick_token(_get_qp(qp_key))
    if parsed:
        an, br = parsed
        # ì „ì—­ ì¸ë±ìŠ¤ íƒìƒ‰
        try:
            gidx = next(i for i, r in df.iterrows() if str(r.get("Analyst", r.get("ì• ë„ë¦¬ìŠ¤íŠ¸",""))) == an and str(r.get("Broker", r.get("ì¦ê¶Œì‚¬",""))) == br)
            sel_gidx = gidx
            st.session_state[sel_key] = sel_gidx
        except StopIteration:
            pass

    # í˜„ì¬ í˜ì´ì§€ ì²´í¬ ìƒíƒœ êµ¬ì„±
    pick_col = "__pick__"
    flags = []
    for i in range(len(view)):
        gidx = start + i
        flags.append(gidx == sel_gidx)
    view.insert(0, pick_col, flags)

    edited = st.data_editor(
        view,
        hide_index=True,
        use_container_width=True,
        height=height,
        disabled=False,  # ì²´í¬ ê°€ëŠ¥
        column_config=column_config or {},
        key=f"{key}_editor_{page}",
    )

    # ì²´í¬ëœ í–‰ íŒë…
    true_idxs = [i for i, v in enumerate(list(edited[pick_col])) if bool(v)]

    new_sel_gidx = sel_gidx
    if len(true_idxs) == 0:
        pass  # ì´ì „ ì„ íƒ ìœ ì§€
    elif len(true_idxs) == 1:
        new_sel_gidx = start + true_idxs[0]
    else:
        prev_local = sel_gidx - start
        candidate = next((i for i in true_idxs if i != prev_local), true_idxs[0])
        new_sel_gidx = start + candidate

    if new_sel_gidx != sel_gidx:
        st.session_state[sel_key] = new_sel_gidx
        # ì¿¼ë¦¬íŒŒë¼ë¯¸í„° ë™ê¸°í™”
        row = df.iloc[new_sel_gidx]
        analyst = str(row.get("Analyst", row.get("ì• ë„ë¦¬ìŠ¤íŠ¸","")))
        broker  = str(row.get("Broker",  row.get("ì¦ê¶Œì‚¬","")))
        _set_qp(qp_key, _build_pick_token(analyst, broker))
        st.rerun()

    # ë°˜í™˜: í˜„ì¬ í˜ì´ì§€ DF(ì„ íƒ ì»¬ëŸ¼ ì œê±°), ì„ íƒí•œ í–‰ dict
    edited_no_pick = edited.drop(columns=[pick_col], errors="ignore")
    picked_row = None
    if 0 <= st.session_state.get(sel_key, -1) < len(df):
        picked_row = df.iloc[st.session_state[sel_key]].to_dict()
    return edited_no_pick, picked_row


# -----------------------------
# Streamlit UI
# -----------------------------
st.title("ğŸ“Š ì¢…ëª©ë¦¬í¬íŠ¸ ë¶„ì„")

# --- Tabs ìŠ¤íƒ€ì¼ ---
st.markdown("""
<style>
div[data-testid="stTabs"] div[role="tablist"] {
  border-bottom: 2px solid #e9ecef; gap: 8px; padding-bottom: 2px; margin-bottom: 8px;
}
div[data-testid="stTabs"] button[role="tab"] {
  font-weight: 600; font-size: 0.95rem; color: #495057;
  background: #f8f9fa; border: 1px solid #e9ecef; border-bottom: none;
  border-top-left-radius: 8px; border-top-right-radius: 8px;
  padding: 10px 14px; box-shadow: inset 0 -3px 0 0 rgba(0,0,0,0.03);
}
div[data-testid="stTabs"] button[role="tab"]:hover { background: #f1f3f5; color: #343a40; }
div[data-testid="stTabs"] button[role="tab"][aria-selected="true"] {
  background: #ffffff; color: #212529; border-color: #dee2e6; box-shadow: inset 0 -4px 0 0 #0d6efd;
}
div[data-testid="stTabs"] > div[data-baseweb="tab-panel"] {
  border: 1px solid #dee2e6; border-top: none; border-radius: 0 10px 10px 10px;
  padding: 16px 16px 10px 16px; background: #ffffff;
}
div[data-testid="stTabs"] button[role="tab"] p { line-height: 1.1; margin: 0; }
</style>
""", unsafe_allow_html=True)

# ---- ì‚¬ì´ë“œë°” ----
with st.sidebar:
    st.header("ê³µí†µ í•„í„°")
    today = dt.date.today()
    default_from = today - dt.timedelta(days=90)
    date_from = st.date_input("From date", value=default_from)
    date_to = st.date_input("To date", value=today)

    metric = st.radio("Ranking metric", ["avg", "sum"], index=0, horizontal=True, key="metric")
    min_reports = st.slider("Minimum reports", 1, 20, 2, 1)
    max_docs = st.slider("Max docs to scan (by_analyst)", 500, 5000, 3000, 100)

    st.caption("â€» ê¸°ê°„/ë¸Œë¡œì»¤ë¥¼ ì¢íìˆ˜ë¡ ë¹ ë¦…ë‹ˆë‹¤. ì ìˆ˜ëŠ” ë¦¬í¬íŠ¸ë³„ points_total í•©ê³„(ë˜ëŠ” í‰ê· )ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    base_docs = load_analyst_docs(date_from, date_to, brokers=None, limit=max_docs)
    broker_counts = {}
    for d in base_docs:
        b = normalize_broker_name((d.get("broker") or "").strip())
        if not b:
            continue
        broker_counts[b] = broker_counts.get(b, 0) + 1
    available_brokers = sorted([b for b, c in broker_counts.items() if c > 0])

    st.markdown("### ì¦ê¶Œì‚¬ ì„ íƒ")
    if not available_brokers:
        st.info("í•´ë‹¹ ê¸°ê°„ì— ë¦¬í¬íŠ¸ê°€ ì¡´ì¬í•˜ëŠ” ì¦ê¶Œì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„ì„ ì¡°ì •í•´ ì£¼ì„¸ìš”.")
        selected_brokers = []
    else:
        labels = [f"{b} ({broker_counts[b]})" for b in available_brokers]
        selected_labels = st.multiselect("ë¦¬í¬íŠ¸ê°€ ì¡´ì¬í•˜ëŠ” ì¦ê¶Œì‚¬ ëª©ë¡", options=labels, default=labels, key="multi_brokers")
        selected_brokers = [lab.split(" (")[0] for lab in selected_labels if " (" in lab]
        if not selected_brokers:
            st.warning("ì„ íƒëœ ì¦ê¶Œì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•´ ì£¼ì„¸ìš”.")

# ---- ë°ì´í„° ë¡œë“œ (ë‘ íƒ­ì—ì„œ ê³µí†µ ì‚¬ìš©) ----
with st.spinner("ë¬¸ì„œ ë¡œë”© ì¤‘..."):
    docs = load_analyst_docs(date_from, date_to, selected_brokers or None, max_docs)

# âš ï¸ ë©´ì±… ì¡°í•­
st.markdown("---")
st.markdown(
    """
    âš ï¸ **ë©´ì±… ì¡°í•­ (Disclaimer)**
    ë³¸ ì‚¬ì´íŠ¸ëŠ” **í•œêµ­í´ë¦¬í…ëŒ€í•™ ìŠ¤ë§ˆíŠ¸ê¸ˆìœµê³¼ í•™ìƒë“¤ì˜ ì‹¤ìŠµ ëª©ì **ìœ¼ë¡œ ì œì‘ëœ ê²ƒì…ë‹ˆë‹¤.
    ë”°ë¼ì„œ ì œê³µë˜ëŠ” ë°ì´í„°ì™€ ë­í‚¹ì€ ì˜¤ë¥˜ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë©°, ì–´ë– í•œ ê³µì‹ ë ¥ë„ ê°–ì§€ ì•ŠìŠµë‹ˆë‹¤.
    ë˜í•œ, ë³¸ ì‚¬ì´íŠ¸ì˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ **íˆ¬ì ê²°ì • ë° ê·¸ ê²°ê³¼ì— ëŒ€í•œ ì±…ì„ì€ ì „ì ìœ¼ë¡œ ì´ìš©ì ë³¸ì¸**ì—ê²Œ ìˆìŠµë‹ˆë‹¤.
    ì œì‘ìëŠ” íˆ¬ì ì†ì‹¤ ë“± ì–´ë– í•œ ë²•ì  ì±…ì„ë„ ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """,
    unsafe_allow_html=True
)

# í‰ê°€ ë°˜ì˜ ê¸°ê°„ ë° ìµœê·¼ ê°±ì‹  ì‹œê°
def get_last_updated_from_docs(docs):
    latest = None
    for d in docs:
        ts = d.get("updated_at")
        if not ts:
            continue
        try:
            t = ts if isinstance(ts, dt.datetime) else ts.to_datetime()
        except Exception:
            try:
                t = dt.datetime.fromisoformat(str(ts))
            except Exception:
                continue
        if latest is None or t > latest:
            latest = t
    return latest

date_values = [str(d.get("report_date", "")).strip() for d in docs if str(d.get("report_date","")).strip()]
if date_values:
    try:
        min_date = min(date_values)
        max_date = max(date_values)
        st.markdown(f"**í‰ê°€ ë°˜ì˜ ê¸°ê°„** {min_date} ~ {max_date}")
    except Exception:
        st.markdown("**í‰ê°€ ë°˜ì˜ ê¸°ê°„** - ~ -")
else:
    st.markdown("**í‰ê°€ ë°˜ì˜ ê¸°ê°„** - ~ -")

st.caption(f"ìµœê·¼ í‰ê°€ ë°˜ì˜ ì‹œê°(ë¬¸ì„œ ê¸°ì¤€): {format_ts(get_last_updated_from_docs(docs))}")

# -----------------------------
# Tabs
# -----------------------------
tab_rank, tab_stock = st.tabs(["ğŸ† ì¢…ëª©ë¦¬í¬íŠ¸ í‰ê°€ ë­í‚¹ë³´ë“œ", "ğŸ” ì¢…ëª©ë³„ ê²€ìƒ‰"])

# ===============================
# íƒ­1: ë­í‚¹ë³´ë“œ â€” ë‹¨ì¼ ì„ íƒ + í•˜ë‹¨ ìƒì„¸
# ===============================
with tab_rank:
    st.subheader("ğŸ† Top Analysts")
    rank_df = make_rank_table(docs, metric=st.session_state.get("metric","avg"), min_reports=min_reports)

    if rank_df.empty:
        st.info("ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„/ë¸Œë¡œì»¤/ìµœì†Œ ë¦¬í¬íŠ¸ ìˆ˜ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")
    else:
        per_page = st.selectbox("Rows per page", [10, 25, 50, 100], index=1, key="rank_rows_per_page")

        view, picked = render_single_select_table(
            rank_df.reset_index(drop=True), key="rank_table",
            page_size=per_page, qp_key="pick",
            height=TABLE_SCROLL_HEIGHT,
            column_config=None
        )

        if picked:
            picked_name = picked.get("Analyst", "Unknown Analyst")
            picked_broker = picked.get("Broker", "Unknown Broker")
            st.markdown(f"### {picked_name} â€” {picked_broker}")
            st.write(f"**Reports:** {picked.get('Reports', '-') } | **RankScore:** {picked.get('RankScore', '-') }")
            st.write(f"**Coverage:** {picked.get('FirstReport', '-') } â†’ {picked.get('LastReport', '-') }")

            with st.spinner("Loading evidence..."):
                items = load_detail_for_analyst(picked_name, picked_broker, date_from, date_to)

            if not items:
                st.info("ìƒì„¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                evidence_rows: List[Dict[str, Any]] = []
                for it in items:
                    evidence_rows.extend(horizon_to_rows(it))
                ev_df = pd.DataFrame(evidence_rows)

                wanted = [
                    "ë³´ê³ ì„œì¼", "ì¢…ëª©", "í‹°ì»¤", "ë ˆì´íŒ…", "ì˜ˆì¸¡í¬ì§€ì…˜", "ì˜ˆì¸¡ëª©í‘œê°€",
                    "horizon(ì¼)",
                    "êµ¬ê°„ìµœê³ ì¢…ê°€", "êµ¬ê°„ìµœì €ì¢…ê°€", "êµ¬ê°„ë§ˆì§€ë§‰ì¢…ê°€", "ëª©í‘œê°€ëŒ€ë¹„ìµœëŒ€ë„ë‹¬%",
                    "ë¦¬í¬íŠ¸ì¼ì— ë§¤ìˆ˜í–ˆì„ê²½ìš° ìˆ˜ìµë¥ %", "ë°©í–¥ì •ë‹µ", "ë°©í–¥ì ìˆ˜", "ëª©í‘œê·¼ì ‘ì ìˆ˜", "ëª©í‘œê°€HIT",
                    "ë¦¬í¬íŠ¸ì´ì ", "ë¦¬í¬íŠ¸PDF", "ìƒì„¸í˜ì´ì§€"
                ]
                cols = [c for c in wanted if c in ev_df.columns] + [c for c in ev_df.columns if c not in wanted]
                ev_df = ev_df[cols]

                st.data_editor(
                    ev_df,
                    hide_index=True,
                    use_container_width=True,
                    height=TABLE_SCROLL_HEIGHT,
                    disabled=True,
                    column_config={
                        "ë¦¬í¬íŠ¸PDF": st.column_config.LinkColumn(label="ë¦¬í¬íŠ¸PDF", display_text="ì—´ê¸°"),
                        "ìƒì„¸í˜ì´ì§€": st.column_config.LinkColumn(label="ìƒì„¸í˜ì´ì§€", display_text="ì—´ê¸°"),
                    },
                    key="rank_evidence_table"
                )
        else:
            st.info("í‘œì—ì„œ í•œ í–‰ì„ ì²´í¬í•˜ë©´ ì•„ë˜ì— ìƒì„¸ê°€ í‘œì‹œë©ë‹ˆë‹¤.")

# ===============================
# íƒ­2: ì¢…ëª©ë³„ ê²€ìƒ‰ â€” ë‹¨ì¼ ì„ íƒ + í•˜ë‹¨ ìƒì„¸
# ===============================
with tab_stock:
    st.subheader("ğŸ” ì¢…ëª©ë³„ ë¦¬í¬íŠ¸ ì¡°íšŒ")
    stock_q = st.text_input("ì¢…ëª©ëª…(ë¶€ë¶„ì¼ì¹˜ í—ˆìš©)", key="stock_q", placeholder="(ê³µë€=ì „ì²´)")
    ticker_q = st.text_input("í‹°ì»¤(ì •í™•íˆ 6ìë¦¬)", key="ticker_q", placeholder="ì˜ˆ: 005930")

    stock_docs = [d for d in docs if match_stock(d, stock_q or None, ticker_q or None)] if (stock_q or ticker_q) else docs
    st.caption(f"ê²€ìƒ‰ê²°ê³¼: {len(stock_docs)}ê±´ (ê¸°ê°„Â·ì¦ê¶Œì‚¬ í•„í„° ì ìš© í›„ ì¢…ëª© í•„í„°)")

    if not stock_docs:
        st.info("í•´ë‹¹ ì¡°ê±´ì˜ ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ëª©ëª…/í‹°ì»¤ ë˜ëŠ” ê¸°ê°„/ì¦ê¶Œì‚¬ë¥¼ ì¡°ì •í•´ ë³´ì„¸ìš”.")
    else:
        detail_rows = []
        title_map_stock = fetch_titles_for_records(stock_docs)
        for r in stock_docs:
            title_safe = _title_from_map_or_fields(r, title_map_stock) or f'{(r.get("stock") or "").strip()} ë¦¬í¬íŠ¸'
            try:
                last_close = last_close_from_horizons(r)
            except Exception:
                last_close = None
            detail_rows.append({
                "ë¦¬í¬íŠ¸ì¼": r.get("report_date",""),
                "ì¦ê¶Œì‚¬": r.get("broker",""),
                "ì• ë„ë¦¬ìŠ¤íŠ¸": r.get("analyst_name") or r.get("analyst",""),
                "ì¢…ëª©": r.get("stock",""),
                "ë ˆì´íŒ…": r.get("rating") or r.get("rating_norm",""),
                "ëª©í‘œê°€": r.get("target_price"),
                "ë§ˆì§€ë§‰ì¢…ê°€": last_close,
                "ì œëª©": title_safe,
            })
        det_df = pd.DataFrame(detail_rows).sort_values("ë¦¬í¬íŠ¸ì¼", ascending=False).reset_index(drop=True)

        page_size2 = st.selectbox("í˜ì´ì§€ í¬ê¸°", options=[25, 50, 100, 200], index=1, key="detail_page_size_v2")
        det_view, det_pick = render_single_select_table(
            det_df, key="stock_detail_main_table_safe",
            page_size=page_size2, qp_key="pick2",
            height=TABLE_SCROLL_HEIGHT,
        )

        if det_pick:
            st.markdown("---")
            st.markdown("#### ğŸ“Œ ì„ íƒ ë¦¬í¬íŠ¸ ìƒì„¸")

            p_stock  = det_pick.get("ì¢…ëª©","")
            p_broker = det_pick.get("ì¦ê¶Œì‚¬","")
            p_anl    = det_pick.get("ì• ë„ë¦¬ìŠ¤íŠ¸","")
            p_date   = det_pick.get("ë¦¬í¬íŠ¸ì¼","")
            p_ticker = ""
            for _r in stock_docs:
                t = str(_r.get("ticker") or "").strip()
                s = (_r.get("stock") or "").strip()
                if s == p_stock and t:
                    p_ticker = t
                    break

            c1, c2, c3, c4 = st.columns([2,2,2,2])
            c1.markdown(f"**ì¢…ëª©/í‹°ì»¤**: {p_stock} ({p_ticker or '-'})")
            c2.markdown(f"**ì¦ê¶Œì‚¬**: {p_broker}")
            c3.markdown(f"**ì• ë„ë¦¬ìŠ¤íŠ¸**: {p_anl}")
            c4.markdown(f"**ë¦¬í¬íŠ¸ì¼**: {p_date}")

            with st.spinner("ì• ë„ë¦¬ìŠ¤íŠ¸ ìƒì„¸ ë¡œë”©..."):
                items = load_detail_for_analyst(p_anl, p_broker, date_from, date_to)
                reports_n = len(items)
                total_pts = 0.0
                dates = []
                for it in items:
                    try:
                        total_pts += float(it.get("points_total") or 0.0)
                    except Exception:
                        pass
                    rd = (it.get("report_date") or "").strip()
                    if rd: dates.append(rd)

                rank_score = (total_pts / reports_n) if reports_n else 0.0
                cov_first = min(dates) if dates else "-"
                cov_last  = max(dates) if dates else "-"
                st.write(f"**Reports:** {reports_n}  |  **RankScore(í‰ê· ):** {round(rank_score, 4) if reports_n else 0.0}  |  **Coverage:** {cov_first} â†’ {cov_last}")

            if items:
                rows_ev = []
                for it in items:
                    rows_ev.extend(horizon_to_rows(it))
                ev_df = pd.DataFrame(rows_ev)

                wanted_cols = [
                    "ë³´ê³ ì„œì¼", "ì¢…ëª©", "í‹°ì»¤", "ë ˆì´íŒ…", "ì˜ˆì¸¡í¬ì§€ì…˜", "ì˜ˆì¸¡ëª©í‘œê°€",
                    "horizon(ì¼)",
                    "êµ¬ê°„ìµœê³ ì¢…ê°€", "êµ¬ê°„ìµœì €ì¢…ê°€", "êµ¬ê°„ë§ˆì§€ë§‰ì¢…ê°€", "ëª©í‘œê°€ëŒ€ë¹„ìµœëŒ€ë„ë‹¬%",
                    "ë¦¬í¬íŠ¸ì¼ì— ë§¤ìˆ˜í–ˆì„ê²½ìš° ìˆ˜ìµë¥ %", "ë°©í–¥ì •ë‹µ", "ë°©í–¥ì ìˆ˜", "ëª©í‘œê·¼ì ‘ì ìˆ˜", "ëª©í‘œê°€HIT",
                    "ë¦¬í¬íŠ¸ì´ì ", "ë¦¬í¬íŠ¸PDF", "ìƒì„¸í˜ì´ì§€"
                ]
                cols = [c for c in wanted_cols if c in ev_df.columns] + [c for c in ev_df.columns if c not in wanted_cols]
                ev_df = ev_df[cols]

                st.data_editor(
                    ev_df,
                    hide_index=True,
                    use_container_width=True,
                    height=TABLE_SCROLL_HEIGHT,
                    disabled=True,
                    column_config={
                        "ë¦¬í¬íŠ¸PDF": st.column_config.LinkColumn(label="ë¦¬í¬íŠ¸PDF", display_text="ì—´ê¸°"),
                        "ìƒì„¸í˜ì´ì§€": st.column_config.LinkColumn(label="ìƒì„¸í˜ì´ì§€", display_text="ì—´ê¸°"),
                    },
                    key="stock_detail_evidence_table_v2"
                )
            else:
                st.info("ì„ íƒí•œ ì• ë„ë¦¬ìŠ¤íŠ¸ì˜ ìƒì„¸ ê·¼ê±°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ìƒì„¸ ëª©ë¡ì—ì„œ í•œ í–‰ì„ ì²´í¬í•˜ë©´, ì•„ë˜ì— í•´ë‹¹ ì• ë„ë¦¬ìŠ¤íŠ¸ì˜ ìƒì„¸ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
