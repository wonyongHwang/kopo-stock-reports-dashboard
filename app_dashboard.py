# -*- coding: utf-8 -*-
"""
Analyst Ranking & Evidence Dashboard (Horizon-only)
- ìƒë‹¨ Top Analysts: í–‰ í´ë¦­(ì²´í¬ë°•ìŠ¤ ì—†ì´)ìœ¼ë¡œ ì„ íƒ â†’ í•˜ë‹¨ì— ìƒì„¸ ê·¼ê±°í‘œ í‘œì‹œ
- 30/60ì¼ ì„±ê³¼ ì„¹ì…˜ ì œê±° (ìš”ì²­ì‚¬í•­)
- Firestore êµ¬ì¡°ëŠ” evaluations/by_analyst ì €ì¥ ìŠ¤í‚¤ë§ˆì— ë§ì¶¤
- ë¸Œë¡œì»¤ëª… ì •ê·œí™”(ê³µë°± ì œê±° ë“±) ì €ì¥Â·ì¡°íšŒ ì „ êµ¬ê°„ì— ì ìš©
"""

import math
import datetime as dt
from typing import List, Dict, Any, Tuple

import streamlit as st
import pandas as pd
import pytz

from google.cloud import firestore
from st_aggrid import DataReturnMode

import re

# -----------------------------
# ì •ê·œí™” & ìœ í‹¸
# -----------------------------
def normalize_broker_name(name: str) -> str:
    """ë¸Œë¡œì»¤ëª… ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±°, í‘œì¤€í™”"""
    if not name:
        return ""
    name = re.sub(r"\s+", "", name)           # ëª¨ë“  ê³µë°± ì œê±°
    name = re.sub(r"[\(\)\[\]]", "", name)    # ê´„í˜¸ë¥˜ ì œê±°
    return name.strip()

def _norm_url(x: str) -> str:
    if not x:
        return ""
    s = str(x).strip()
    if s.startswith(("http://", "https://")):
        return s
    return "https://" + s

KST = pytz.timezone("Asia/Seoul")

def format_ts(ts, tz=KST):
    """Firestore Timestamp ë˜ëŠ” ë¬¸ìì—´/None â†’ ë³´ê¸°ìš© ë¬¸ìì—´"""
    if not ts:
        return "-"
    try:
        dt_utc = ts if isinstance(ts, dt.datetime) else ts.to_datetime()
    except Exception:
        try:
            dt_utc = dt.datetime.fromisoformat(str(ts))
        except Exception:
            return str(ts)
    if dt_utc.tzinfo is None:
        dt_utc = dt_utc.replace(tzinfo=dt.timezone.utc)
    return dt_utc.astimezone(tz).strftime("%Y-%m-%d %H:%M:%S %Z")

# --- (ì¶”ê°€) ì„ íƒ í–‰ ì¸ë±ìŠ¤ ì•ˆì „ ì¶”ì¶œ í—¬í¼ ---
def _extract_selected_idx_from_aggrid(grid_ret) -> int | None:
    sel = grid_ret.get("selected_rows", [])
    # DataFrame ì¼€ì´ìŠ¤
    if isinstance(sel, pd.DataFrame):
        if sel.empty:
            return None
        return int(sel.iloc[0]["_row"])
    # list[dict] ì¼€ì´ìŠ¤
    if isinstance(sel, list):
        if not sel:
            return None
        row0 = sel[0]
        try:
            return int(row0.get("_row"))
        except AttributeError:
            return int(row0["_row"])
    return None

# ====== ì„ íƒ UI: streamlit-aggrid ìœ ë¬´ì— ë”°ë¥¸ ë¶„ê¸° ======
_AGGRID_AVAILABLE = True
try:
    from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode
except Exception:
    _AGGRID_AVAILABLE = False

# -----------------------------
# ì „ì—­/í—¬í¼
# -----------------------------
EXCLUDED_BROKERS = {"í•œêµ­IRí˜‘ì˜íšŒ"}  # ì œì™¸í•  ì¦ê¶Œì‚¬ëª… ì§‘í•©

def _parse_report_date(val) -> dt.date | None:
    """report_dateê°€ ë¬¸ìì—´/íƒ€ì„ìŠ¤íƒ¬í”„/None ë“± ì„ì—¬ ìˆì–´ë„ ì•ˆì „í•˜ê²Œ dateë¡œ ë³€í™˜"""
    try:
        if hasattr(val, "to_datetime"):
            val = val.to_datetime()
        if isinstance(val, dt.datetime):
            return val.date()
        s = (str(val) or "").strip()
        if not s:
            return None
        # ìš°ì„  ISO yyyy-mm-dd ìš°ì„ 
        if len(s) == 10 and s[4] == "-" and s[7] == "-":
            return dt.date.fromisoformat(s)
        # ëŠìŠ¨í•œ í¬ë§·ë„ ìˆ˜ìš©
        return dt.date.fromisoformat(pd.to_datetime(s).date().isoformat())
    except Exception:
        return None

def filter_docs_by_date(docs: list[dict], date_from: dt.date | None, date_to: dt.date | None) -> list[dict]:
    """report_date ê¸°ì¤€ìœ¼ë¡œ í•œ ë²ˆ ë” í´ë¼ì´ì–¸íŠ¸ í•„í„°(ë­í‚¹ ì „ìš© ì•ˆì „ì¥ì¹˜)"""
    if not (date_from or date_to):
        return docs
    out = []
    for x in docs:
        d = _parse_report_date(x.get("report_date"))
        if d is None:
            continue
        if date_from and d < date_from:
            continue
        if date_to and d > date_to:
            continue
        out.append(x)
    return out


def is_allowed_broker(broker: str) -> bool:
    # ë¹„êµë„ ì •ê·œí™”ê°’ìœ¼ë¡œ
    return normalize_broker_name(broker) not in {normalize_broker_name(b) for b in EXCLUDED_BROKERS}

# -----------------------------
# Firestore Client & Caches
# -----------------------------
@st.cache_resource(show_spinner=False)
def get_db():
    return firestore.Client()

@st.cache_data(show_spinner=False, ttl=60)
def load_broker_list() -> List[str]:
    """by_analystì—ì„œ brokerë§Œ ëª¨ì•„ ì •ê·œí™” í›„ ìœ ë‹ˆí¬ ë°˜í™˜"""
    db = get_db()
    q = db.collection_group("by_analyst").select(["broker"]).limit(5000)
    s = set()
    try:
        for d in q.stream():
            b = (d.to_dict() or {}).get("broker")
            b = normalize_broker_name(b)
            if b and is_allowed_broker(b):
                s.add(b)
    except Exception:
        pass
    return sorted(s)

@st.cache_data(show_spinner=False, ttl=60)
def load_analyst_docs(min_date: dt.date | None, max_date: dt.date | None,
                      brokers: List[str] | None,
                      limit: int = 3000) -> List[Dict[str, Any]]:
    """
    by_analyst ë¬¸ì„œ ë¡œë“œ (í•„ìš”í•œ í•„í„°ë§Œ ì„œë²„ ì¸¡ where, ë‚˜ë¨¸ì§€ëŠ” í´ë¼ì´ì–¸íŠ¸ í•„í„°)
    brokers: ì •ê·œí™”ëœ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ëŒ€
    """
    db = get_db()
    q = db.collection_group("by_analyst")
    try:
        if min_date: q = q.where("report_date", ">=", min_date.isoformat())
        if max_date: q = q.where("report_date", "<=", max_date.isoformat())
        q = q.limit(limit)
        docs = list(q.stream())
    except Exception:
        # ì¸ë±ìŠ¤ ì´ìŠˆ ì‹œ ë²”ìœ„ë¥¼ ì¤„ì—¬ì„œë¼ë„ ì½ê¸°
        q = db.collection_group("by_analyst").limit(limit)
        docs = list(q.stream())

    out: List[Dict[str, Any]] = []
    broker_set = {normalize_broker_name(b) for b in (brokers or [])} or None
    for d in docs:
        x = d.to_dict() or {}
        # ì •ê·œí™”
        broker_raw = (x.get("broker") or "").strip()
        broker_norm = normalize_broker_name(broker_raw)
        # ì œì™¸
        if not is_allowed_broker(broker_norm):
            continue
        # ì„ íƒ í•„í„°
        if broker_set and broker_norm not in broker_set:
            continue

        x["broker"] = broker_norm   # ì •ê·œí™”ê°’ìœ¼ë¡œ í†µì¼
        # êµ¬í˜• í˜¸í™˜: points_total ì—†ìœ¼ë©´ horizon í•©ìœ¼ë¡œ ëŒ€ì²´
        if x.get("points_total") is None and x.get("horizons"):
            x["points_total"] = sum(float(h.get("points_total_this_horizon") or 0.0) for h in x.get("horizons") or [])
        out.append(x)
    return out

@st.cache_data(show_spinner=False, ttl=60)
def load_detail_for_analyst(analyst_name: str, broker: str,
                            date_from: dt.date | None, date_to: dt.date | None,
                            limit: int = 800) -> List[Dict[str, Any]]:
    """
    íŠ¹ì • ì• ë„ë¦¬ìŠ¤íŠ¸(ì´ë¦„+ë¸Œë¡œì»¤)ì˜ by_analyst ë¬¸ì„œ ëª©ë¡ ë¡œë“œ
    - ì •ê·œí™”ëœ brokerë¡œ 1ì°¨ ì¡°íšŒ
    - (ê³¼ê±°ë°ì´í„° í˜¸í™˜) ì •ê·œí™” ì „ brokerê°€ ë‹¤ë¥´ë©´ 2ì°¨ ì¡°íšŒ í›„ ë³‘í•©
    """
    db = get_db()
    out: List[Dict[str, Any]] = []

    broker_norm = normalize_broker_name(broker)

    def _query(b: str) -> List[Dict[str, Any]]:
        q = (db.collection_group("by_analyst")
               .where("broker", "==", normalize_broker_name(b))
               .where("analyst_name", "==", analyst_name))
        try:
            if date_from: q = q.where("report_date", ">=", date_from.isoformat())
            if date_to:   q = q.where("report_date", "<=", date_to.isoformat())
        except Exception:
            pass
        q = q.limit(limit)
        docs = [z.to_dict() or {} for z in q.stream()]
        # ë¸Œë¡œì»¤ ì •ê·œí™” ë³´ì •
        for z in docs:
            z["broker"] = normalize_broker_name(z.get("broker"))
        return docs

    # 1ì°¨: ì •ê·œí™” ê°’ìœ¼ë¡œ
    out.extend(_query(broker_norm))
    # 2ì°¨: ë¹„ì •ê·œí™”ê°€ ë‹¤ë¥¼ ê²½ìš°(ê³¼ê±° ë¬¸ì„œ í˜¸í™˜)
    if broker != broker_norm:
        out.extend(_query(broker))  # ì¤‘ë³µ ê°€ëŠ¥

    # ì¤‘ë³µ ì œê±°(report_id, pdf_url ê¸°ì¤€ ìš°ì„ )
    seen: set[Tuple[str, str]] = set()
    uniq: List[Dict[str, Any]] = []
    for r in out:
        key = (str(r.get("report_id","")), str(r.get("pdf_url","")))
        if key in seen:
            continue
        seen.add(key)
        uniq.append(r)

    uniq.sort(key=lambda r: r.get("report_date",""), reverse=True)
    return uniq

# -----------------------------
# Helpers (í‘œí˜„/ì§‘ê³„)
# -----------------------------
def _price_stats(price_sample: list[dict]) -> tuple[float|None, float|None, float|None]:
    if not price_sample:
        return None, None, None
    closes = [float(p.get("close")) for p in price_sample if p.get("close") is not None]
    if not closes:
        return None, None, None
    max_c = max(closes)
    min_c = min(closes)
    last_c = closes[-1]
    return max_c, min_c, last_c

def normalize_name(s: str) -> str:
    return (s or "").strip()

def _sign(x: float | None) -> int | None:
    if x is None:
        return None
    if x > 0: return 1
    if x < 0: return -1
    return 0

def _desired_from_horizon(h: Dict[str, Any]) -> int | None:
    dtg = h.get("direction_target") or {}
    desired = dtg.get("desired")
    try:
        return int(desired) if desired is not None else None
    except Exception:
        return None

def format_pos_label(desired: int | None, rating_norm: str | None) -> str:
    r = (rating_norm or "").lower()
    if desired == 1 or r == "buy":  return "ìƒìŠ¹(buy)"
    if desired == -1 or r == "sell": return "í•˜ë½(sell)"
    return "ì¤‘ë¦½(hold)"

def horizon_to_rows(rec: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    by_analyst í•œ ë¬¸ì„œ(= í•œ ë¦¬í¬íŠ¸) â†’ ì—¬ëŸ¬ horizon ê·¼ê±° í–‰ìœ¼ë¡œ í¼ì¹¨
    """
    rows = []
    horizons = rec.get("horizons") or []
    rating_norm = rec.get("rating_norm") or rec.get("rating")
    target_price = rec.get("target_price")

    for h in horizons:
        desired = _desired_from_horizon(h)
        ret = h.get("return_pct", None)  # start_close ëŒ€ë¹„ end_close ìˆ˜ìµë¥ 
        actual_sign = _sign(ret)
        correct = None
        if desired is not None and actual_sign is not None:
            if desired == 0:
                correct = (actual_sign == 0)
            else:
                correct = (desired == actual_sign)

        price_sample = h.get("price_sample") or []
        max_c, min_c, last_c = _price_stats(price_sample)

        # ëª©í‘œê°€ ëŒ€ë¹„ ìˆ˜ìµë¥  (ë§ˆì§€ë§‰ ì¢…ê°€ ê¸°ì¤€)
        target_return_pct = None
        if target_price and target_price not in ("", 0, 0.0) and last_c:
            try:
                target_return_pct = round((float(last_c) - float(target_price)) / float(target_price) * 100, 3)
            except Exception:
                target_return_pct = None

        # ëª©í‘œê°€ ëŒ€ë¹„ ìµœëŒ€ ë„ë‹¬ë¥  (ìµœê³ ê°€ ê¸°ì¤€)
        max_reach_pct = None
        if target_price and target_price not in ("", 0, 0.0) and max_c:
            try:
                max_reach_pct = round((float(max_c) / float(target_price) - 1) * 100, 3)
            except Exception:
                max_reach_pct = None

        rows.append({
            "ë³´ê³ ì„œì¼": rec.get("report_date",""),
            "horizon(ì¼)": h.get("horizon_days"),
            "ì˜ˆì¸¡í¬ì§€ì…˜": format_pos_label(desired, rating_norm),
            "ë ˆì´íŒ…": rating_norm,
            "ì˜ˆì¸¡ëª©í‘œê°€": target_price,

            # ê¸°ì¡´ ìˆ˜ìµë¥  (ë¦¬í¬íŠ¸ ì‹œì‘ ëŒ€ë¹„)
            "ë¦¬í¬íŠ¸ì¼ì— ë§¤ìˆ˜í–ˆì„ê²½ìš° ìˆ˜ìµë¥ %": None if ret is None else round(ret*100, 3),

            # ëª©í‘œê°€ ëŒ€ë¹„
            "ëª©í‘œê°€ëŒ€ë¹„ìˆ˜ìµë¥ %": target_return_pct,
            "ëª©í‘œê°€ëŒ€ë¹„ìµœëŒ€ë„ë‹¬%": max_reach_pct,

            "ë°©í–¥ì •ë‹µ": None if correct is None else ("âœ…" if correct else "âŒ"),
            "ë°©í–¥ì ìˆ˜": h.get("direction_point", 0.0),
            "ëª©í‘œê·¼ì ‘ì ìˆ˜": h.get("target_proximity_point", 0.0),
            "ëª©í‘œê°€HIT": h.get("hit_target_anytime", None),

            "êµ¬ê°„ìµœê³ ì¢…ê°€": max_c,
            "êµ¬ê°„ìµœì €ì¢…ê°€": min_c,
            "êµ¬ê°„ë§ˆì§€ë§‰ì¢…ê°€": last_c,

            "ì¢…ëª©": rec.get("stock",""),
            "í‹°ì»¤": rec.get("ticker",""),
            "ì œëª©": rec.get("title",""),
            "ë¦¬í¬íŠ¸PDF": rec.get("pdf_url","") or rec.get("report_url",""),
            "ìƒì„¸í˜ì´ì§€": rec.get("detail_url",""),
            "ë¦¬í¬íŠ¸ì´ì ": rec.get("points_total", 0.0),
        })
    return rows

def make_rank_table(docs: list[dict], metric: str = "avg", min_reports: int = 2) -> pd.DataFrame:
    from collections import defaultdict
    agg = defaultdict(lambda: {"sum":0.0, "n":0, "name":"", "broker":"", "first":None, "last":None})

    if not docs:
        return pd.DataFrame(columns=["RankScore","Reports","Analyst","Broker","FirstReport","LastReport"])

    # ì§‘ê³„
    for x in docs:
        name = (x.get("analyst_name") or "").strip() or "(unknown)"
        broker = normalize_broker_name((x.get("broker") or "").strip())
        try:
            pts = float(x.get("points_total") or 0.0)
        except Exception:
            pts = 0.0
        rdate = (x.get("report_date") or "").strip()

        key = f"{name}|{broker}"
        agg[key]["sum"] += pts
        agg[key]["n"] += 1
        agg[key]["name"] = name
        agg[key]["broker"] = broker
        if rdate:
            if not agg[key]["first"] or rdate < agg[key]["first"]:
                agg[key]["first"] = rdate
            if not agg[key]["last"] or rdate > agg[key]["last"]:
                agg[key]["last"]  = rdate

    rows = []
    for _, v in agg.items():
        if v["n"] < min_reports:
            continue
        score = (v["sum"]/v["n"]) if metric == "avg" else v["sum"]
        rows.append({
            "RankScore": round(score, 4),
            "Reports": v["n"],
            "Analyst": v["name"],
            "Broker": v["broker"],
            "FirstReport": v["first"] or "",
            "LastReport": v["last"] or "",
        })

    if not rows:
        return pd.DataFrame(columns=["RankScore","Reports","Analyst","Broker","FirstReport","LastReport"])

    df = pd.DataFrame(rows)
    if not {"RankScore","Reports"}.issubset(df.columns):
        return df.reset_index(drop=True)
    return df.sort_values(["RankScore","Reports"], ascending=[False, False]).reset_index(drop=True)

def get_last_updated_from_docs(docs):
    latest = None
    for d in docs:
        ts = d.get("updated_at")
        if ts is None:
            continue
        try:
            t = ts if isinstance(ts, dt.datetime) else ts.to_datetime()
        except Exception:
            try:
                t = dt.datetime.fromisoformat(str(ts))
            except Exception:
                continue
        if latest is None or t > latest:
            latest = t
    return latest

# -----------------------------
# Streamlit UI
# -----------------------------
st.set_page_config(page_title="í•œêµ­í´ë¦¬í…ëŒ€í•™ ìŠ¤ë§ˆíŠ¸ê¸ˆìœµê³¼", layout="wide")
st.title("ğŸ“Š ì¢…ëª©ë¦¬í¬íŠ¸ í‰ê°€ ë­í‚¹ë³´ë“œ")

# ---- ë©´ì±… ì¡°í•­ ----
st.markdown("---")
st.markdown(
    """
    âš ï¸ **ë©´ì±… ì¡°í•­ (Disclaimer)**
    ë³¸ ì‚¬ì´íŠ¸ëŠ” **í•œêµ­í´ë¦¬í…ëŒ€í•™ ìŠ¤ë§ˆíŠ¸ê¸ˆìœµê³¼ í•™ìƒë“¤ì˜ ì‹¤ìŠµ ëª©ì **ìœ¼ë¡œ ì œì‘ëœ ê²ƒì…ë‹ˆë‹¤.
    ë”°ë¼ì„œ ì œê³µë˜ëŠ” ë°ì´í„°ì™€ ë­í‚¹ì€ ì˜¤ë¥˜ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë©°, ì–´ë– í•œ ê³µì‹ ë ¥ë„ ê°–ì§€ ì•ŠìŠµë‹ˆë‹¤.
    ë˜í•œ, ë³¸ ì‚¬ì´íŠ¸ì˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ **íˆ¬ì ê²°ì • ë° ê·¸ ê²°ê³¼ì— ëŒ€í•œ ì±…ì„ì€ ì „ì ìœ¼ë¡œ ì´ìš©ì ë³¸ì¸ì—ê²Œ** ìˆìŠµë‹ˆë‹¤.
    ì œì‘ìëŠ” íˆ¬ì ì†ì‹¤ ë“± ì–´ë– í•œ ë²•ì  ì±…ì„ë„ ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """,
    unsafe_allow_html=True
)

# ---- ì‚¬ì´ë“œë°” ----
with st.sidebar:
    st.header("Filters")
    today = dt.date.today()
    default_from = today - dt.timedelta(days=365)
    date_from = st.date_input("From date", value=default_from)
    date_to = st.date_input("To date", value=today)

    metric = st.radio("Ranking metric", ["avg", "sum"], index=0, horizontal=True)
    min_reports = st.slider("Minimum reports", 1, 20, 2, 1)
    max_docs = st.slider("Max docs to scan (by_analyst)", 500, 5000, 3000, 100)
    st.caption("â€» ê¸°ê°„/ë¸Œë¡œì»¤ë¥¼ ì¢íìˆ˜ë¡ ë¹ ë¦…ë‹ˆë‹¤. ì ìˆ˜ëŠ” ë¦¬í¬íŠ¸ë³„ points_total í•©ê³„(ë˜ëŠ” í‰ê· )ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # ê¸°ê°„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œ ë¡œë“œ â†’ ì´ìš© ê°€ëŠ¥í•œ ë¸Œë¡œì»¤ ì§‘í•©/ê±´ìˆ˜ ê³„ì‚°
    base_docs = load_analyst_docs(date_from, date_to, brokers=None, limit=max_docs)
    broker_counts = {}
    for d in base_docs:
        b = normalize_broker_name((d.get("broker") or "").strip())
        if not b:
            continue
        broker_counts[b] = broker_counts.get(b, 0) + 1

    available_brokers = sorted([b for b, c in broker_counts.items() if c > 0])

    st.markdown("### ì¦ê¶Œì‚¬ ì„ íƒ")

    if not available_brokers:
        st.info("í•´ë‹¹ ê¸°ê°„ì— ë¦¬í¬íŠ¸ê°€ ì¡´ì¬í•˜ëŠ” ì¦ê¶Œì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„ì„ ì¡°ì •í•´ ì£¼ì„¸ìš”.")
        selected_brokers = []
    else:
        # í‘œì‹œìš© ë¼ë²¨: "ì¦ê¶Œì‚¬ëª… (ë¦¬í¬íŠ¸ìˆ˜)"
        display_labels = [f"{b} ({broker_counts[b]})" for b in available_brokers]
        # ê¸°ë³¸ ì„ íƒ: ì „ì²´ ì„ íƒ ìƒíƒœ
        default_selection = display_labels

        # ë©€í‹° ì…€ë ‰íŠ¸ UI
        selected_labels = st.multiselect(
            "ë¦¬í¬íŠ¸ê°€ ì¡´ì¬í•˜ëŠ” ì¦ê¶Œì‚¬ ëª©ë¡",
            options=display_labels,
            default=default_selection,
            help="Ctrl/Cmd í´ë¦­ìœ¼ë¡œ ë‹¤ì¤‘ ì„ íƒ, ì „ì²´ ì„ íƒ ë˜ëŠ” í•´ì œ ê°€ëŠ¥"
        )

        # ì„ íƒëœ ë¼ë²¨ì—ì„œ ì‹¤ì œ broker ì´ë¦„ë§Œ ì¶”ì¶œ
        selected_brokers = [
            b.split(" (")[0] for b in selected_labels if " (" in b
        ]

        if not selected_brokers:
            st.warning("ì„ íƒëœ ì¦ê¶Œì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•´ ì£¼ì„¸ìš”.")


# ---- ë°ì´í„° ë¡œë“œ ----
with st.spinner("Loading analyst documents..."):
    docs = load_analyst_docs(date_from, date_to, selected_brokers or None, max_docs)

last_updated = get_last_updated_from_docs(docs)
st.markdown(f"ìµœê·¼ í‰ê°€ ë°˜ì˜ ì‹œê°(ë¬¸ì„œ ê¸°ì¤€): {format_ts(last_updated)}")

# í‰ê°€ ë°˜ì˜ ê¸°ê°„ í‘œì‹œ
try:
    _date_vals = [str(d.get("report_date", "")).strip() for d in docs]
    _date_vals = [s for s in _date_vals if s and len(s) >= 8]
    if _date_vals:
        _min_date = min(_date_vals)
        _max_date = max(_date_vals)
        st.markdown(f"í‰ê°€ ë°˜ì˜ ê¸°ê°„ {_min_date} ~ {_max_date}")
    else:
        st.markdown("í‰ê°€ ë°˜ì˜ ê¸°ê°„ - ~ -")
except Exception:
    st.markdown("í‰ê°€ ë°˜ì˜ ê¸°ê°„ - ~ -")

# ---- ìƒë‹¨: Top Analysts (í˜ì´ì§€ë„¤ì´ì…˜ ê·¸ëŒ€ë¡œ) ----
st.subheader("ğŸ† Top Analysts")

# ğŸ‘‡ ë­í‚¹ ì§‘ê³„ìš©ìœ¼ë¡œ í•œ ë²ˆ ë” í™•ì‹¤í•˜ê²Œ ë‚ ì§œ í•„í„° ê°•ì œ
docs_for_rank = filter_docs_by_date(docs, date_from, date_to)

# ë””ë²„ê·¸: í•„í„° ì „/í›„ ê±´ìˆ˜ í™•ì¸
st.caption(f"[DEBUG] ranking docs: before={len(docs)} after_date_filter={len(docs_for_rank)}")

rank_df = make_rank_table(docs_for_rank, metric=metric, min_reports=min_reports)


if rank_df.empty:
    st.info("ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„/ë¸Œë¡œì»¤ í•„í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")
    st.stop()

per_page = st.selectbox("Rows per page", [10,25,50,100], 1)
page = st.number_input("Page", 1, 9999, 1)
total_pages = max(1, math.ceil(len(rank_df)/per_page))
page = min(page, total_pages)
start, end = (page-1)*per_page, (page-1)*per_page + per_page
show_df = rank_df.iloc[start:end].copy()

st.caption(f"Page {page}/{total_pages} Â· Total analysts: {len(rank_df)}")

st.markdown("---")
st.subheader("ğŸ” í•˜ë‹¨ ìƒì„¸: ìƒë‹¨ í‘œì—ì„œ í–‰ì„ í´ë¦­í•˜ë©´ ìë™ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤")

# ---- í•µì‹¬: í–‰ ì„ íƒ UI (Ag-Grid) ----
selected_idx = None
_show_df = show_df.reset_index(drop=True).copy()

# í–‰/í—¤ë” ë†’ì´ ì„¤ì •
ROW_H = 34
HEAD_H = 40
PADDING = 32
GRID_H = HEAD_H + ROW_H * len(_show_df) + PADDING   # í˜„ì¬ í˜ì´ì§€ì— ë³´ì—¬ì¤„ í–‰ ìˆ˜ë§Œí¼
# 25í–‰ì´ ëª¨ë‘ ë³´ì´ë„ë¡ ì¶©ë¶„íˆ í‚¤ìš°ë˜, ê³¼ë„í•œ ë†’ì´ ë°©ì§€
GRID_H = min(GRID_H, HEAD_H + ROW_H * 25 + 400)      # í•„ìš”ì‹œ ìƒë‹¨ ìƒìˆ˜ ì¡°ì ˆ

if _AGGRID_AVAILABLE and not _show_df.empty:
    _show_df.insert(0, "_row", _show_df.index)

    gb = GridOptionsBuilder.from_dataframe(_show_df)
    # âœ… ë‹¨ì¼ ì„ íƒ (ì²´í¬ë°•ìŠ¤ ì—†ìŒ)
    gb.configure_selection(selection_mode="single", use_checkbox=False)

    # âŒ autoHeight ì œê±° (ë‚´ë¶€ ìŠ¤í¬ë¡¤ ì‚¬ìš©)
    # gb.configure_grid_options(domLayout="autoHeight")
    gb.configure_grid_options(
        rowHeight=ROW_H,
        headerHeight=HEAD_H,
        # í•„ìš”ì‹œ: suppressHorizontalScroll=True,
    )
    gb.configure_column("_row", header_name="", hide=True)

    grid = AgGrid(
        _show_df,
        gridOptions=gb.build(),
        update_mode=GridUpdateMode.SELECTION_CHANGED,
        data_return_mode=DataReturnMode.AS_INPUT,
        theme="streamlit",
        allow_unsafe_jscode=True,
        fit_columns_on_grid_load=True,
        height=GRID_H,     # ğŸ”· per_pageì— ë§ì¶˜ ì¶©ë¶„í•œ ë†’ì´
    )

    selected_idx = _extract_selected_idx_from_aggrid(grid)

else:
    # ---- Fallback: st.dataframe + ë“œë¡­ë‹¤ìš´ ì„ íƒ (ë™ì¼ ë†’ì´ ì ìš©) ----
    st.info("ê³ ê¸‰ í–‰ í´ë¦­ ì„ íƒì„ ìœ„í•´ `streamlit-aggrid` ì„¤ì¹˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤. (fallback UI ì‚¬ìš© ì¤‘)")
    df_to_show = _show_df.drop(columns=["_row"], errors="ignore") if "_row" in _show_df else _show_df
    st.dataframe(df_to_show, use_container_width=True, height=GRID_H)  # ğŸ”· ë™ì¼ ë†’ì´
    options = [f"{i}: {row['Analyst']} â€” {row['Broker']} (RankScore={row['RankScore']})"
               for i, row in _show_df.iterrows()]
    if options:
        pick_label = st.selectbox("ìƒì„¸ë¥¼ ë³¼ í–‰ ì„ íƒ (ëŒ€ì²´ UI)", options, index=0)
        try:
            selected_idx = int(pick_label.split(":")[0])
        except Exception:
            selected_idx = 0


# ---- ì„ íƒ ì—†ìœ¼ë©´ ì•ˆë‚´ ----
if selected_idx is None:
    st.info("ìƒë‹¨ í‘œì—ì„œ í–‰ì„ í´ë¦­(ì„ íƒ)í•˜ì„¸ìš”.")
    st.stop()

# ---- ì„ íƒëœ ì• ë„ë¦¬ìŠ¤íŠ¸ ì •ë³´ í‘œì‹œ + ìƒì„¸ ê·¼ê±°í‘œ ----
picked = _show_df.iloc[selected_idx]
picked_name = picked.get("Analyst", "Unknown Analyst")
picked_broker = picked.get("Broker", "Unknown Broker")

st.markdown(f"### {picked_name} â€” {picked_broker}")
st.write(f"**Reports:** {picked.get('Reports', '-') } | **RankScore:** {picked.get('RankScore', '-') }")
st.write(f"**Coverage:** {picked.get('FirstReport', '-') } â†’ {picked.get('LastReport', '-') }")

with st.spinner("Loading evidence..."):
    items = load_detail_for_analyst(picked_name, picked_broker, date_from, date_to)

if not items:
    st.info("ìƒì„¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# ë¦¬í¬íŠ¸ë³„ Horizon ê·¼ê±°ë¥¼ ê¸¸ê²Œ í¼ì³ì„œ í•œ í…Œì´ë¸”ë¡œ êµ¬ì„±
evidence_rows: List[Dict[str, Any]] = []
for it in items:
    evidence_rows.extend(horizon_to_rows(it))

if not evidence_rows:
    st.warning("Horizon ê·¼ê±°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
else:
    ev_df = pd.DataFrame(evidence_rows)

    # ë³´ê¸° ì¢‹ê²Œ ì»¬ëŸ¼ ìˆœì„œ ì¬ë°°ì¹˜
    wanted_cols = [
        "ë³´ê³ ì„œì¼", "ì¢…ëª©", "í‹°ì»¤", "ë ˆì´íŒ…", "ì˜ˆì¸¡í¬ì§€ì…˜", "ì˜ˆì¸¡ëª©í‘œê°€",
        "horizon(ì¼)",
        "êµ¬ê°„ìµœê³ ì¢…ê°€", "êµ¬ê°„ìµœì €ì¢…ê°€", "êµ¬ê°„ë§ˆì§€ë§‰ì¢…ê°€", "ëª©í‘œê°€ëŒ€ë¹„ìµœëŒ€ë„ë‹¬%",
        "ë¦¬í¬íŠ¸ì¼ì— ë§¤ìˆ˜í–ˆì„ê²½ìš° ìˆ˜ìµë¥ %", "ë°©í–¥ì •ë‹µ", "ë°©í–¥ì ìˆ˜", "ëª©í‘œê·¼ì ‘ì ìˆ˜", "ëª©í‘œê°€HIT",
        "ë¦¬í¬íŠ¸ì´ì ", "ì œëª©", "ë¦¬í¬íŠ¸PDF", "ìƒì„¸í˜ì´ì§€"
    ]
    cols = [c for c in wanted_cols if c in ev_df.columns] + [c for c in ev_df.columns if c not in wanted_cols]
    ev_df = ev_df[cols]

    # ë§í¬ ì»¬ëŸ¼ ì •ë¦¬ (ìŠ¤í‚´ ëˆ„ë½ ëŒ€ë¹„)
    if "ë¦¬í¬íŠ¸PDF" in ev_df.columns:
        ev_df["ë¦¬í¬íŠ¸PDF"] = ev_df["ë¦¬í¬íŠ¸PDF"].map(_norm_url)
    if "ìƒì„¸í˜ì´ì§€" in ev_df.columns:
        ev_df["ìƒì„¸í˜ì´ì§€"] = ev_df["ìƒì„¸í˜ì´ì§€"].map(_norm_url)

    # Streamlit ìµœì‹  ë²„ì „: LinkColumn ì‚¬ìš©
    try:
        st.dataframe(
            ev_df,
            use_container_width=True,
            column_config={
                "ë¦¬í¬íŠ¸PDF": st.column_config.LinkColumn(
                    label="ë¦¬í¬íŠ¸PDF",
                    help="PDF ì—´ê¸°",
                    display_text="ì—´ê¸°"
                ),
                "ìƒì„¸í˜ì´ì§€": st.column_config.LinkColumn(
                    label="ìƒì„¸í˜ì´ì§€",
                    help="ìƒì„¸ í˜ì´ì§€ ì—´ê¸°",
                    display_text="ì—´ê¸°"
                ),
            },
        )
    except Exception:
        # êµ¬ë²„ì „ í´ë°±: HTML í…Œì´ë¸”
        def _a(href):
            return f'<a href="{href}" target="_blank" rel="noopener noreferrer">ì—´ê¸°</a>' if href else ""
        html_df = ev_df.copy()
        if "ë¦¬í¬íŠ¸PDF" in html_df.columns:
            html_df["ë¦¬í¬íŠ¸PDF"] = html_df["ë¦¬í¬íŠ¸PDF"].map(_a)
        if "ìƒì„¸í˜ì´ì§€" in html_df.columns:
            html_df["ìƒì„¸í˜ì´ì§€"] = html_df["ìƒì„¸í˜ì´ì§€"].map(_a)

        st.markdown(html_df.to_html(escape=False, index=False), unsafe_allow_html=True)

    st.caption("â€» í•œêµ­í´ë¦¬í…ëŒ€í•™ ìŠ¤ë§ˆíŠ¸ê¸ˆìœµê³¼ ì‹¤ìŠµ ëª©ì ì˜ ê²°ê³¼ë¬¼ë¡œ ë°ì´í„° ì·¨í•©, ì •ì œ, ë¶„ì„ì´ ë¶€ì •í™• í•  ìˆ˜ ìˆìŒ.")
